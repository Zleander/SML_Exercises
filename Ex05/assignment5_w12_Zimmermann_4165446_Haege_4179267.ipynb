{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96d2211a5a01c5d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5 {-}\n",
    "## Due May 23 at 12:00 {-}\n",
    "\n",
    "Please note: \n",
    "\n",
    "- Read the instructions in the exercise PDF and in this notebook carefully.\n",
    "- Add your solutions *only* at `YOUR CODE HERE`/`YOUR ANSWER HERE` and remove the corresponding `raise NotImplementedError()`.\n",
    "- Do not chance the provided code and text, if not stated.\n",
    "- Do not *add* or *delete* cells.\n",
    "- Do not `import` additional functionality. \n",
    "- Before submitting: Please make sure, that your notebook can be executed from top to bottom `Menu -> Kernel -> Restart & Run all`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Lagrange multipliers, 2 points)\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\max x+y \\\\\n",
    "    &\\text{s.t. } x^2+2y^2 \\leq 5 \\\\\n",
    "    L(x, y, \\alpha) &= (x+y) - \\alpha (x^2+2y^2-5) \\\\\n",
    "    \\frac{\\partial L}{\\partial x} &= 1  - 2\\alpha x \\overset{!}{=} 0 \\\\\n",
    "    \\alpha &= \\frac{1}{2x} \\\\\n",
    "    \\frac{\\partial L}{\\partial y} &= 1  - 4\\alpha y \\overset{!}{=} 0 \\\\\n",
    "    \\alpha &= \\frac{1}{4y} \\\\\n",
    "    \\frac{1}{2x} &= \\frac{1}{4y} \\\\\n",
    "    x &= 2y \\\\\n",
    "    L(y,\\alpha) &= (2y+y) - \\alpha ((2y)^2+2y^2-5) \\\\\n",
    "    \\frac{\\partial L}{\\partial \\alpha} &= (2y)^2+2y^2-5 \\overset{!}{=} 0\\\\\n",
    "    4y^2 + 2y^2 &= 5 \\\\\n",
    "    y^2 &= \\frac{5}{6} \\\\\n",
    "    y &= \\sqrt{\\frac{5}{6}} \\\\\n",
    "    x &= 2\\sqrt{\\frac{5}{6}} = \\sqrt{\\frac{10}{3}} \\\\\n",
    "    \\alpha &= \\frac{1}{2x} \\geq 0 \\\\\n",
    "\\end{align*}\n",
    "The constraint is active. Geometrically this can be explained, by thinking about the uncontrained solution of $\\max x+y$ and the ellipsoid, that $x^2+2y^2=5$ describes. We can then easily see, that the solution of the constrained problem lies at a point on said ellipsoid, not inside it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (Linear and quadratic programs, 3+1+1=5 points)\n",
    "## a)\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\min_{x\\in \\mathbb{R}^n} c^T x \\\\\n",
    "    \\text{s.t. }\n",
    "    &Ax \\leq b \\\\\n",
    "    &x \\geq 0 \\\\\\\\\n",
    "    \\iff &\\begin{pmatrix}A\\\\-I\\end{pmatrix}x \\leq \\begin{pmatrix}b\\\\0\\end{pmatrix} \\\\\\\\\n",
    "    L(\\lambda, x) &= c^Tx + \\lambda^T \\left(\\begin{pmatrix}A\\\\-I\\end{pmatrix}x - \\begin{pmatrix}b\\\\0\\end{pmatrix}\\right) \\\\\n",
    "    &= c^Tx + \\lambda^T \\begin{pmatrix}A\\\\-I\\end{pmatrix}x - \\lambda^T \\begin{pmatrix}b\\\\0\\end{pmatrix} \\\\\\\\\n",
    "    \\frac{\\partial L}{\\partial x} &= c^T - \\lambda^T \\begin{pmatrix}A\\\\-I\\end{pmatrix} \\overset{!}{=} 0 \\\\\n",
    "    0&=c^T - \\lambda^T \\begin{pmatrix}A\\\\-I\\end{pmatrix} \\\\\n",
    "    0&=c^Tx - \\lambda^T \\begin{pmatrix}A\\\\-I\\end{pmatrix}x \\\\\\\\\n",
    "    \\text{Dual:} \\\\\n",
    "    g(\\lambda) &= -\\lambda^T \\begin{pmatrix}b\\\\0\\end{pmatrix} \\\\\\\\\n",
    "    \\iff \\\\\\\\\n",
    "    \\max_{\\lambda}& - \\lambda^T \\begin{pmatrix}b\\\\0\\end{pmatrix} \\\\\n",
    "    \\text{s.t. }\n",
    "    &c^Tx - \\lambda^T \\begin{pmatrix}A\\\\-I\\end{pmatrix}x = 0\n",
    "\\end{align*}\n",
    "\n",
    "$\\implies $ Dual is also linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) \n",
    "\\begin{align*}\n",
    "    E &= \\frac{1}{2}x^TQx + c^Tx \\\\\n",
    "    \\frac{\\partial E}{\\partial x} &= x^TQ + c \\\\\n",
    "    \\frac{\\partial^2 E}{\\partial x^2} &= Q\n",
    "\\end{align*}\n",
    "When we derive the problem twice, we can see the $Q$ is the Hessian matrix. Therefore we require $Q$ to be positive semi-definite for the problem to be convex.\n",
    "\n",
    "## c)\n",
    "\n",
    "This means that, as the solutions for the dual and the primal are equal, it does not matter wether we solve the primal or the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (Primal hard margin SVM problem, 3 points)\n",
    "\n",
    "Any solution of this problem is subject to:  \n",
    "$$Y_i(w^TX_i+b)\\geq1 \\;\\forall i=1,\\dots,n$$  \n",
    "If the hyperplane is not in canonical representation, it holds:\n",
    "$$\\min_{i} |w^TX_i+b| \\neq 1 $$  \n",
    "And thus, because $Y_i \\in {-1,1}$:  \n",
    "$$Y_i(w^TX_i+b)>1 \\;\\forall i=1,\\dots,n$$  \n",
    "Then $$\\exists \\hat w \\text{ with } \\frac{1}{2}\\|\\hat w\\|^2 < \\frac{1}{2}\\| w\\|^2 $$  \n",
    "in other words, $w$ does not minimize the objective, and is not a solution.  \n",
    "While $$Y_i(\\hat w^TX_i+b)\\geq1 \\;\\forall i=1,\\dots,n$$\n",
    "and $$\\exists i\\; Y_i(\\hat w^TX_i+b)=1 $$  \n",
    "and therefore $$\\min_{i} |\\hat{w}^TX_i+b| = 1$$\n",
    "in other words, $\\hat w$ is a valid solution and a canonical Hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffb10fbc071fc814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.testing import assert_equal, assert_almost_equal\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#  Hide warnings of LinearSVC, LogisticRegression\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65c9e677a5c1d059",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## a) Know the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0da6208813948939",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- **Features:** \n",
    "    30 different tissue measurements of the samples (like concavitivity, radius, ...)\n",
    "- **Labels:** \n",
    "    malignant/benign {1,0} tells if the sample contains harmful tissue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f454aa4ab0e7abc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "\n",
    "xs = dataset.data\n",
    "ys = dataset.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `xs, ys` to the training set `xs_train, ys_train` (size $70\\%$) and the test set `xs_test, ys_test` (size $30\\%$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a440cbfbbb0a67b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b)/c) Optimize the Hyperparameters: Linear SVM vs Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7d3b8a7e75f7895",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can do better than  0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "# center and normalize\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(xs)\n",
    "xs = scaler.transform(xs)\n",
    "\n",
    "# split into training and test test\n",
    "n_train = int(len(xs) * .7)\n",
    "\n",
    "xs_train, xs_test = xs[:n_train], xs[n_train:]\n",
    "ys_train, ys_test = ys[:n_train], ys[n_train:]\n",
    "\n",
    "# evaluate standard SVM\n",
    "svm_estimator = LinearSVC().fit(xs_train,ys_train)\n",
    "print('We can do better than ', np.mean(svm_estimator.predict(xs_test) != ys_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find good hyperparameters *without* the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0493355d0168522f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params Logistic Regression:  {'C': 0.40370172585965536, 'Error': 0.03136867088607595}\n",
      "best params Linear SVC:  {'C': 0.003511191734215131, 'Error': 0.028243670886075933}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT USE xs_test, ys_test here!\n",
    "# Please make your optimization reproducable (e.g. set random_state, seed, â€¦)\n",
    "# LinearSVC and LogisticRegression\n",
    "\n",
    "#best_params_svm, best_params_lr = {}, {}\n",
    "folds = 5\n",
    "Cs=np.logspace(-3,3,100)\n",
    "\n",
    "\n",
    "def get_best_params(xs_train, ys_train, Cs, folds, rdm_state = 42):\n",
    "    svc_error = 10\n",
    "    logreg_error = 10\n",
    "    svc_C = 0\n",
    "    logreg_C = 0\n",
    "    \n",
    "    \n",
    "    for c in Cs:\n",
    "        svc_err = []\n",
    "        logreg_err = []\n",
    "        for i in list(range(0,folds - 1)):\n",
    "            # Split the folds in train and validation set\n",
    "            x_test = xs_train[int(i*(len(xs_train)/folds)): int((i+1)*(len(xs_train)/folds))]\n",
    "            x_train = np.append(xs_train[:int(i*len(xs_train)/folds)],xs_train[int((i+1)*len(xs_train)/folds):], axis=0)\n",
    "            y_test = ys_train[int(i * len(ys_train)/folds): int((i+1)*len(ys_train)/folds)]\n",
    "            y_train = np.append(ys_train[:int(i*len(ys_train)/folds):],ys_train[int((i+1)*len(ys_train)/folds):])\n",
    "\n",
    "            svc = LinearSVC(random_state=rdm_state, C=c).fit(x_train, y_train)\n",
    "            logreg=LogisticRegression(random_state=rdm_state,C=c).fit(x_train,y_train)\n",
    "\n",
    "            svc_err.append(1-svc.score(x_test,y_test))\n",
    "            logreg_err.append(1-logreg.score(x_test,y_test))\n",
    "        svc_err = np.mean(svc_err)\n",
    "        logreg_err = np.mean(logreg_err)\n",
    "        if logreg_err < logreg_error:\n",
    "            logreg_error = logreg_err\n",
    "            logreg_C = c\n",
    "        if svc_err < svc_error:\n",
    "            svc_error = svc_err\n",
    "            svc_C = c\n",
    "    return {'C': svc_C, 'Error': svc_error}, {'C': logreg_C, 'Error': logreg_error}\n",
    "\n",
    "best_params_svm, best_params_lr = get_best_params(xs_train, ys_train, Cs, folds)    \n",
    "\n",
    "print(\"best params Logistic Regression: \",best_params_lr)\n",
    "print(\"best params Linear SVC: \",best_params_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-213d9fdca48368f0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SCV Error:  0.011695906432748537\n",
      "Logistic Regressen Error:  0.0292397660818714\n"
     ]
    }
   ],
   "source": [
    "# if you did not solve the cross validation, use this:\n",
    "# svm_estimator = LinearSVC(C=0.0035, random_state=42).fit(xs_train, ys_train)\n",
    "# lr_estimator = LogisticRegression(C=0.4037,random_state=42).fit(xs_train, ys_train)\n",
    "# else, use this:\n",
    "svm_estimator = LinearSVC(random_state=42, C=best_params_svm['C']).fit(xs_train, ys_train)\n",
    "lr_estimator = LogisticRegression(random_state=42,C=best_params_lr['C']).fit(xs_train,ys_train)\n",
    "\n",
    "svc_error = (1-svm_estimator.score(xs_test,ys_test))\n",
    "logreg_error = (1-lr_estimator.score(xs_test,ys_test))\n",
    "\n",
    "print('Linear SCV Error: ', svc_error)\n",
    "print('Logistic Regressen Error: ' ,logreg_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-563e6d1de9e41fbf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "What do you observe and why?\n",
    "\n",
    "Answer: SVC performs way worse on test set than on validation sets. For all other folds the validation sets are trained as well. Therefore the validation set is not independent of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7fd6a82e440e12c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) State concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5afbd54df2ee393d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Ethical:** \n",
    "    It is an ethical issue to only look at the classification error because it implicitly assumes that false positives and false negatives are equally bad, when in fact it is way worse to not detect if a patient has cancer \n",
    "\n",
    "2. **Technical/Statistical:**\n",
    "    dataset does not have an representative proportion of malignant/benigne samples. In real life the proportion of actually cancerous tissue is way less than benigne tissue.\n",
    "\n",
    "3. **Any:**\n",
    "    The classification does not contain a measure of certainty. Samples close to the decision boundary are treated the same as clear cases \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
